import re
import os
import pickle

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å TensorFlow
try:
    import tensorflow as tf
    import numpy as np
    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import LabelEncoder
    TENSORFLOW_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è TensorFlow –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")
    print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: pip install tensorflow numpy pandas scikit-learn")
    TENSORFLOW_AVAILABLE = False
    # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è —Ç–∏–ø–æ–≤
    tf = None
    np = None
    pd = None
    train_test_split = None
    LabelEncoder = None

class NeuralChemistryPredictor:
    """–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ö–∏–º–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∞–∫—Ü–∏–π"""

    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.label_encoder = None
        self.max_length = 50
        self.vocab_size = 1000
        self.model_path = 'neural_chemistry_model.h5'
        self.tokenizer_path = 'tokenizer.pkl'
        self.label_encoder_path = 'label_encoder.pkl'
        self.tensorflow_available = TENSORFLOW_AVAILABLE

    def create_training_data(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Ä–µ–∞–∫—Ü–∏–π"""
        reactions = [
            # –ú–µ—Ç–∞–ª–ª + –∫–∏—Å–ª–æ—Ç–∞
            ("Li + HCl", "LiCl + H2"),
            ("Na + HCl", "NaCl + H2"),
            ("K + HCl", "KCl + H2"),
            ("Ca + HCl", "CaCl2 + H2"),
            ("Mg + HCl", "MgCl2 + H2"),
            ("Zn + HCl", "ZnCl2 + H2"),
            ("Fe + HCl", "FeCl2 + H2"),
            ("Al + HCl", "AlCl3 + H2"),

            # –ú–µ—Ç–∞–ª–ª + –∫–∏—Å–ª–æ—Ä–æ–¥
            ("Li + O2", "Li2O"),
            ("Na + O2", "Na2O"),
            ("K + O2", "K2O"),
            ("Ca + O2", "CaO"),
            ("Mg + O2", "MgO"),
            ("Zn + O2", "ZnO"),
            ("Fe + O2", "Fe2O3"),
            ("Al + O2", "Al2O3"),
            ("Cu + O2", "CuO"),

            # –ö–∏—Å–ª–æ—Ç–∞ + –æ—Å–Ω–æ–≤–∞–Ω–∏–µ
            ("HCl + NaOH", "NaCl + H2O"),
            ("H2SO4 + NaOH", "Na2SO4 + H2O"),
            ("HNO3 + NaOH", "NaNO3 + H2O"),
            ("HCl + KOH", "KCl + H2O"),
            ("H2SO4 + KOH", "K2SO4 + H2O"),
            ("HCl + Ca(OH)2", "CaCl2 + H2O"),

            # –ì–æ—Ä–µ–Ω–∏–µ
            ("C + O2", "CO2"),
            ("CH4 + O2", "CO2 + H2O"),
            ("C2H6 + O2", "CO2 + H2O"),
            ("C3H8 + O2", "CO2 + H2O"),
            ("H2 + O2", "H2O"),

            # –†–∞–∑–ª–æ–∂–µ–Ω–∏–µ
            ("CaCO3", "CaO + CO2"),
            ("Cu(OH)2", "CuO + H2O"),
            ("H2O2", "H2O + O2"),
            ("KClO3", "KCl + O2"),

            # –í—ã—Ç–µ—Å–Ω–µ–Ω–∏–µ
            ("Zn + CuSO4", "ZnSO4 + Cu"),
            ("Fe + CuSO4", "FeSO4 + Cu"),
            ("Al + CuSO4", "Al2(SO4)3 + Cu"),
        ]

        return reactions

    def tokenize_formula(self, formula):
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ö–∏–º–∏—á–µ—Å–∫–æ–π —Ñ–æ—Ä–º—É–ª—ã"""
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —ç–ª–µ–º–µ–Ω—Ç—ã, —á–∏—Å–ª–∞ –∏ —Å–∫–æ–±–∫–∏
        tokens = re.findall(r'[A-Z][a-z]*|\d+|[()+\-\s]', formula)
        return ' '.join(tokens)

    def prepare_data(self, reactions):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        if not self.tensorflow_available:
            return None, None

        X_texts = []
        y_texts = []

        for reactants, products in reactions:
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ä–µ–∞–≥–µ–Ω—Ç—ã
            reactant_tokens = self.tokenize_formula(reactants)
            X_texts.append(reactant_tokens)

            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–¥—É–∫—Ç—ã
            product_tokens = self.tokenize_formula(products)
            y_texts.append(product_tokens)

        # –°–æ–∑–¥–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        all_texts = X_texts + y_texts
        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(
            num_words=self.vocab_size,
            oov_token='<OOV>'
        )
        self.tokenizer.fit_on_texts(all_texts)

        # –ö–æ–¥–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        X_sequences = self.tokenizer.texts_to_sequences(X_texts)
        X_padded = tf.keras.preprocessing.sequence.pad_sequences(
            X_sequences, maxlen=self.max_length, padding='post'
        )

        # –î–ª—è —Ü–µ–ª–µ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º one-hot encoding
        self.label_encoder = LabelEncoder()
        y_encoded = self.label_encoder.fit_transform(y_texts)
        y_categorical = tf.keras.utils.to_categorical(y_encoded)

        return X_padded, y_categorical

    def build_model(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏"""
        if not self.tensorflow_available or self.label_encoder is None:
            return None

        model = tf.keras.Sequential([
            tf.keras.layers.Embedding(
                input_dim=self.vocab_size,
                output_dim=128,
                input_length=self.max_length
            ),
            tf.keras.layers.LSTM(128, return_sequences=True),
            tf.keras.layers.LSTM(64),
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(len(self.label_encoder.classes_), activation='softmax')
        ])

        model.compile(
            optimizer='adam',
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )

        self.model = model
        return model

    def train(self, epochs=50, batch_size=16):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        if not self.tensorflow_available:
            print("‚ùå TensorFlow –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –û–±—É—á–µ–Ω–∏–µ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ.")
            return None

        print("–°–æ–∑–¥–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
        reactions = self.create_training_data()
        X, y = self.prepare_data(reactions)

        print(f"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞: {len(X)} –ø—Ä–∏–º–µ—Ä–æ–≤")

        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        print("–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        self.build_model()

        print("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        history = self.model.fit(
            X_train, y_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_data=(X_test, y_test),
            verbose=1
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã
        self.save_model()

        print("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        return history

    def predict(self, reactants):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ —Ä–µ–∞–∫—Ü–∏–∏"""
        if not self.tensorflow_available:
            print("‚ùå TensorFlow –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∞–≤–∏–ª–æ–≤–æ–π –ø–æ–¥—Ö–æ–¥.")
            return None

        if self.model is None:
            if not self.load_model():
                return None

        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        reactant_tokens = self.tokenize_formula(reactants)
        sequence = self.tokenizer.texts_to_sequences([reactant_tokens])
        padded = tf.keras.preprocessing.sequence.pad_sequences(
            sequence, maxlen=self.max_length, padding='post'
        )

        # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º
        prediction = self.model.predict(padded, verbose=0)
        predicted_index = np.argmax(prediction[0])

        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        predicted_products = self.label_encoder.inverse_transform([predicted_index])[0]

        return predicted_products

    def save_model(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤"""
        try:
            self.model.save(self.model_path)
            with open(self.tokenizer_path, 'wb') as f:
                pickle.dump(self.tokenizer, f)
            with open(self.label_encoder_path, 'wb') as f:
                pickle.dump(self.label_encoder, f)
            print("–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")

    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤"""
        try:
            if os.path.exists(self.model_path):
                self.model = tf.keras.models.load_model(self.model_path)
                with open(self.tokenizer_path, 'rb') as f:
                    self.tokenizer = pickle.load(f)
                with open(self.label_encoder_path, 'rb') as f:
                    self.label_encoder = pickle.load(f)
                print("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                return True
            else:
                print("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                return False
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False

    def get_model_info(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        if not self.tensorflow_available:
            return """
‚ùå TensorFlow –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω

ü§ñ –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞

üì¶ –î–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –ò–ò —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
pip install tensorflow numpy pandas scikit-learn

üîÑ –ü–æ–∫–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–æ–≤–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ä–µ–∞–∫—Ü–∏–π
            """

        if self.model is None:
            return """
ü§ñ –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ä–µ–∞–∫—Ü–∏–π

üìä –°—Ç–∞—Ç—É—Å: –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞
üí° –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /train –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏

üîÑ –°–µ–π—á–∞—Å —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–æ–≤–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º
            """

        info = f"""
ü§ñ –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ä–µ–∞–∫—Ü–∏–π

üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏:
‚Ä¢ –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {self.vocab_size}
‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {self.max_length}
‚Ä¢ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: LSTM (128 ‚Üí 64)
‚Ä¢ –ê–∫—Ç–∏–≤–∞—Ü–∏—è: ReLU + Softmax

üéØ –¢–æ—á–Ω–æ—Å—Ç—å: –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ {len(self.create_training_data())} –ø—Ä–∏–º–µ—Ä–∞—Ö —Ä–µ–∞–∫—Ü–∏–π
        """

        return info

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥
def predict_with_neural_network(reactants):
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –≤ –±–æ—Ç–µ"""
    predictor = NeuralChemistryPredictor()

    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å
    if not predictor.load_model():
        print("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –æ–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏...")
        predictor.train(epochs=30)
        print("–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞!")

    prediction = predictor.predict(reactants)
    return prediction

if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    predictor = NeuralChemistryPredictor()

    if predictor.tensorflow_available:
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        print("üöÄ –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏...")
        predictor.train(epochs=30)

        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        test_reactions = ["Zn + HCl", "CH4 + O2", "Na + O2"]
        print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
        for reaction in test_reactions:
            prediction = predictor.predict(reaction)
            print(f"–í—Ö–æ–¥: {reaction} ‚Üí –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {prediction}")
    else:
        print("‚ùå TensorFlow –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")
        print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∫–æ–º–∞–Ω–¥–æ–π:")
        print("pip install tensorflow numpy pandas scikit-learn")
        print("\nüîÑ –ü–æ–∫–∞ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª–æ–≤–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º –≤ –±–æ—Ç–µ.")